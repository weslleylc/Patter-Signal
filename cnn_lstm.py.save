# lstm autoencoder recreate sequence
import numpy as np
import pandas as pd
from matmodel import utils, feature_builder as fb
import matplotlib.pyplot as plt
import tensorflow as tf
from scipy.signal import butter, lfilter
from keras.models import Sequential
from keras.layers import Dense, Activation, Conv2D, Dropout, LSTM, Conv1D, MaxPooling1D, GRU, MaxPool2D, ELU
from keras.layers import MaxPooling2D, Flatten, RepeatVector, TimeDistributed, Reshape, BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.models import model_from_json
import cv2


def save(filepath, fig=None):
    '''Save the current image with no whitespace
    Example filepath: "myfig.png" or r"C:\myfig.pdf" 
    '''
    import matplotlib.pyplot as plt
    if not fig:
        fig = plt.gcf()

    plt.subplots_adjust(0,0,1,1,0,0)
    for ax in fig.axes:
        ax.axis('off')
        ax.margins(0,0)
        ax.xaxis.set_major_locator(plt.NullLocator())
        ax.yaxis.set_major_locator(plt.NullLocator())
    fig.savefig(filepath, pad_inches = 0, bbox_inches='tight')

def get_images_from_signals(array, width, heigth, directory = "images/"):
    images = []
    for count, i in enumerate(df.T):
        fig = plt.figure()
        plt.plot(i)
        plt.xticks([]), plt.yticks([])
        plt.ylim(-1, 1.5)
#        plt.xlim(0,70)
        for spine in plt.gca().spines.values():
            spine.set_visible(False)
            
        plt.subplots_adjust(0,0,1,1,0,0)
        for ax in fig.axes:
            ax.axis('off')
            ax.margins(0,0)
            ax.xaxis.set_major_locator(plt.NullLocator())
            ax.yaxis.set_major_locator(plt.NullLocator())
        plt.show()
        filename = directory  + str(count)+'.png'
        fig.savefig(filename, pad_inches = 0, bbox_inches='tight')
        im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
        im_gray = cv2.resize(im_gray, (width, heigth), interpolation = cv2.INTER_LANCZOS4)
        ret, thresh = cv2.threshold(im_gray, 127, 255, 
                                    cv2.THRESH_BINARY | cv2.THRESH_OTSU)
        cv2.imwrite(filename, thresh)
        plt.close()
        
        images.append(thresh)
    return np.array(images).T

def read_images_from_signals(n, directory = "images/"):
    images = []
    for count in range(n):
        filename = directory  + str(count)+'.png'
        im_gray = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)
        ret, thresh = cv2.threshold(im_gray, 127, 255, 
                                    cv2.THRESH_BINARY | cv2.THRESH_OTSU)
        images.append(thresh)
    return np.array(images).T



dataset = np.load('./data/morfology.npy').item()
n = 600
fs = 360
list_ecg = dataset['signal'][:n]
list_peaks = dataset['peaks'][:n]
list_labels = dataset['label'][:n]
list_labels, b = pd.factorize(list_labels)
list_of_packets = []
for (sig, peak, label) in zip(list_ecg, list_peaks, list_labels):
    sig == np.subtract(sig, np.mean(sig))
    list_of_packets.append(fb.LabeledPacket(np.array(sig), peak, label))

width = 350
heigth = 680
my_ecg = fb.ECG.build_from_packets(list_of_packets)
len_packet = 35
list_ecg = my_ecg.build_packets(len_packet)
list_ecg = [x.input_signal for x in my_ecg.packets]
df = pd.DataFrame(list_ecg).values
df = df.T
input_shape = df.shape
n_in = df.shape[0]
y = df
#X = get_images_from_signals(df, width, heigth)
X = read_images_from_signals(n)
# (batch, time, width, height, channel)
#X = X.reshape(600, 1, width, heigth, 1)
X = X.reshape(600, width, heigth, 1)
X = X/255  # normalize
y = df.reshape(600, 70)
input_shape = X.shape


## create generator to standardize images
#datagen = ImageDataGenerator(rescale=1.0/255.0, featurewise_center=True, featurewise_std_normalization=True)
## calculate mean on training dataset
#datagen.fit(X)
## prepare an iterators to scale images
#train_iterator = datagen.flow(X, y, batch_size=64)
#test_iterator = datagen.flow(X, y, batch_size=64)


#CNN 2d#
model = Sequential()

model.add(Conv2D(32, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), padding="same", activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(200, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(70, activation='relu'))
model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=5000, verbose=1)

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")

#model.fit(X, y, epochs=10, initial_epoch = 8, verbose=1)
#yy = model.predict(X[0:1,...])
#yy = yy.flatten()
#plt.plot(yy)


#DNN#
#model = Sequential()
#model.add(Dense(units=400, input_dim=784, activation='relu'))
#model.add(Dense(units=400, activation='relu'))
##
#
#model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),
#                 activation='relu',
#                 input_shape=input_shape[1:]))
#model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
#model.add(Conv2D(64, (5, 5), activation='relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))
#model.add(Flatten())
#model.add(Dense(1000, activation='relu'))
#model.add(Dense(70, activation='relu'))
#model.compile(optimizer='adam', loss='mse')
#model.fit(X, y, epochs=5, verbose=1)
#yy = model.predict(X)
#




#
#
#
#model = Sequential()
#
#model.add(
#    TimeDistributed(
#        Conv2D(32, (3, 3), activation='relu'), 
#        input_shape=(1, width, heigth, 1)
#    )
#)
#model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(1, 1))))
#
##model.add(TimeDistributed(Conv2D(64, (4,4), activation='relu')))
##model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))
#
## extract features and dropout 
#model.add(TimeDistributed(Flatten()))
#model.add(Dropout(0.5))
#
## input to LSTM
#model.add(LSTM(2, return_sequences=False, dropout=0.5))
#
## classifier with sigmoid activation for multilabel
#model.add(Dense(70))
#model.compile(optimizer='adam', loss='mse')
#
#model.fit(X, y, epochs=5, verbose=1)
#


#
## building the mode
#model = Sequential()
#model.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),
#                 activation='relu',
#                 input_shape=(1, 432, 288, 1))))
##model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))))
##model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))
#model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))
#model.add(TimeDistributed(Flatten()))
##model.add(TimeDistributed(Dense(n_in)))
#model.add(LSTM(100, activation='relu', input_shape=(n_in, 1)))
##model.add(RepeatVector(n_in))
##model.add(LSTM(100, activation='relu', return_sequences=True))
#model.add(TimeDistributed(Dense(70)))
#model.compile(optimizer='adam', loss='mse')
## fit model
#model.fit(X, y, epochs=10, verbose=0)
#print('Accurracy: {}'.format(scores[1])) 

